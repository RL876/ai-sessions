{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9154358e-d49f-4e04-94f9-a3e0dc2d0772",
   "metadata": {},
   "source": [
    "## DNN - Dense Neural Networks (Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12880b33-90d1-4bca-8df4-d9fb35d2e203",
   "metadata": {},
   "source": [
    "Learn the features from data.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*63sGPbvLLpvlD16hG1bvmA.gif)\n",
    "\n",
    "Assume we have 'm' examples with 'n' parameters and 'k' categories.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "Given \\quad \\mathbb{X}_ {(m,n)}&=\\begin{vmatrix}\n",
    "x_ {11} & x_ {12} & ... & x_ {1n}\\\\\n",
    "x_ {21} & x_ {22} & ... & x_ {2n}\\\\\n",
    "| & | & ... & |\\\\\n",
    "x_ {m1} & x_ {m2} & ... & x_ {mn}\n",
    "\\end{vmatrix}\n",
    "\\\\\n",
    "\\\\\n",
    "Given\\quad \\mathbb{Y}_ {(m,1)}&=\\begin{vmatrix}\n",
    "y_ {1}\\\\\n",
    "y_ {2}\\\\\n",
    "|\\\\\n",
    "y_ {m}\n",
    "\\end{vmatrix}=\\begin{vmatrix}\n",
    "cat\\\\\n",
    "dog\\\\\n",
    "|\\\\\n",
    "cat\n",
    "\\end{vmatrix}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3787a4a3-7c37-4314-84c3-ac88446987f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.loadtxt(\"sample_data/mnist_train_small.csv\", delimiter=\",\", dtype=int)\n",
    "X = arr[1:, 1:]\n",
    "Y = arr[1:, 0]\n",
    "print(f\"X: {len(X)}\\n\", X, \"\\n\", X[0])\n",
    "print(f\"Y: {len(Y)}\\n\", Y, \"\\n\", Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404bbe1-9c5b-4e88-ae32-8adaf752d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = X[0]\n",
    "print(Y[0])\n",
    "Image.fromarray(img.reshape(28, 28).astype(np.uint8)).resize((140, 140)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad7e14-0ff7-4adb-9407-fbc420c186c1",
   "metadata": {},
   "source": [
    "## Onehot Encoding\n",
    "\n",
    "$$\\mathbb{Y}_ {(m,k)}=\\begin{vmatrix}\n",
    "y_ {11} & y_ {12} & ... & y_ {1k}\\\\\n",
    "y_ {21} & y_ {22} & ... & y_ {2k}\\\\\n",
    "| & | & ... & |\\\\\n",
    "y_ {m1} & y_ {m2} & ... & y_ {mk}\n",
    "\\end{vmatrix}=\\begin{vmatrix}\n",
    "1 & 0 & ... & 0\\\\\n",
    "0 & 1 & ... & 0\\\\\n",
    "| & | & ... & |\\\\\n",
    "1 & 0 & ... & 0\n",
    "\\end{vmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68ad18-edef-4c2e-9f16-811df3d246fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(y):\n",
    "    labels = np.unique(y)\n",
    "    labels.sort()\n",
    "    label_dict = dict()\n",
    "    for value in labels:\n",
    "        key = np.where(labels == value)[0][0]\n",
    "        label_dict[key] = str(value)\n",
    "        y = np.where(y == value, key, y)\n",
    "    y = y.astype(int)\n",
    "    y = np.eye(len(np.unique(y)))[y].astype(int)\n",
    "    return y, label_dict\n",
    "\n",
    "Y_onehot, label_dict = oneHot(Y)\n",
    "X_train, X_test = X[:18000, :], X[18000:, :]\n",
    "Y_train, Y_test = Y_onehot[:18000], Y_onehot[18000:]\n",
    "Y_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a74d54-10f6-4871-90e2-65b1eafcd09f",
   "metadata": {},
   "source": [
    "## Weight and bias\n",
    "\n",
    "![](https://c.mql5.com/18/20/NN1__1.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c9bec-b93d-4315-be9d-4b8c68be10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(784, 128) * 0.1\n",
    "b1 = np.random.randn(128) * 0.1\n",
    "W2 = np.random.randn(128, 64) * 0.1\n",
    "b2 = np.random.randn(64) * 0.1\n",
    "W3 = np.random.randn(64, 10) * 0.1\n",
    "b3 = np.random.randn(10) * 0.1\n",
    "print(\"parameters:\", (784 * 128) + (128 * 64) + (64 * 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca576b8-629d-4343-8c91-482430efd38b",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ZafDv3VUm60Eh10OeJu1vw.png)\n",
    "\n",
    "* sigmoid function\n",
    "\n",
    "$$\\sigma(z)=\\cfrac{1}{1+e^{-z}}=a$$\n",
    "    \n",
    "$$\\sigma'(z)=\\sigma(z)(1-\\sigma(z))=a(1-a)$$\n",
    "\n",
    "* tanh (second choise)\n",
    "    \n",
    "$$\\sigma(z)=tanh(z)=\\cfrac{e^{z}-e^{-z}}{e^{z}+e^{-z}}=a$$\n",
    "    \n",
    "$$\\sigma'(z)=1-(tanh(z))^2=1-a^2$$\n",
    "\n",
    "* ReLU (most people using for non-negative values)\n",
    "    \n",
    "$$\\sigma(z)=max(0,z)$$\n",
    "\n",
    "$$\\sigma'(z)=\\begin{cases}0,\\quad if\\ z<0\\\\\n",
    "1,\\quad if\\ z\\ge0\n",
    "\\end{cases}$$\n",
    "\n",
    "* Leaky ReLU\n",
    "\n",
    "$$\\sigma(z)=max(0.01z,z)$$\n",
    "\n",
    "$$\\sigma'(z)=\\begin{cases}0.01,\\quad if\\ z<0\\\\\n",
    "1,\\quad \\quad \\ if\\ z\\ge0\n",
    "\\end{cases}$$\n",
    "\n",
    "* SoftMax\n",
    "\n",
    "$$\\sigma(z)=\\cfrac{e^{z}}{\\sum{e^{z}}}=a$$\n",
    "\n",
    "$$\\sigma'(z)=Y_{hat}-z$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438146b-1b41-47db-b1ac-14e2004d82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_backward(dz):\n",
    "    return 1 * (dz > 0)\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    z_exp = np.exp(z)\n",
    "    y_hat = z_exp / np.sum(z_exp, axis=1, keepdims=True)\n",
    "    y_hat = np.where(y_hat == 0, 10**-10, y_hat)\n",
    "    return y_hat\n",
    "\n",
    "def softmax_backward(y_hat, Y):\n",
    "    return y_hat - Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84732ca8-59b3-4a7f-b9e7-ccc7d54b6726",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "$$\\begin{aligned}\n",
    "X^{[0]}_ {(m,n)}&=X_ {(m,n)}\\\\\n",
    "\\\\\n",
    "X^{[1]}_ {(m, h_ {1})}&=\\sigma^{[1]}(Z^{[1]}_ {(m, h_ {1})})=\\sigma^{[1]}(X^{[0]}_ {(m,n)}{\\ \\cdot\\ }W^{[1]}_ {(n,h_ {1})}+B^{[1]}_ {(1,h_ {1})})\\\\\n",
    "\\\\\n",
    "X^{[2]}_ {(m, h_ {2})}&=\\sigma^{[2]}(Z^{[2]}_ {(m, h_ {2})})=\\sigma^{[2]}(X^{[1]}_ {(m, h_ {1})}{\\ \\cdot\\ }W^{[2]}_ {(h_ {1},h_ {2})}+B^{[2]}_ {(1,h_ {2})})\\\\\n",
    "\\\\\n",
    "&\\bullet\\\\\n",
    "&\\bullet\\\\\n",
    "&\\bullet\\\\\n",
    "\\\\\n",
    "X^{[l]}_ {(m, h_ {l})}&=\\sigma^{[l]}(Z^{[l]}_ {(m, h_ {l})})=\\sigma^{[l]}(X^{[l-1]}_ {(m, h_ {l-1})}{\\ \\cdot\\ }W^{[l]}_ {(h_ {l-1},h_ {l})}+B^{[l]}_ {(1,h_ {l})})\\\\\n",
    "\\\\\n",
    "\\hat{Y}_ {(m,k)}&=g(Z^{[o]}_ {(m,k)})=g^{[o]}(X^{[l]}_ {(m, h_ {l})}{\\ \\cdot\\ }W^{[o]}_ {(h_ {l},k)}+B^{[o]}_ {(1,k)})\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d27937-a22a-44c4-b4c5-3c95b834020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = np.dot(X_train, W1) + b1\n",
    "X1 = relu(Z1)\n",
    "Z2 = np.dot(X1, W2) + b2\n",
    "X2 = relu(Z2)\n",
    "Z3 = np.dot(X2, W3) + b3\n",
    "Y_hat = softmax(Z3)\n",
    "Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e2187-475c-41a9-8932-21f7d6760974",
   "metadata": {},
   "source": [
    "## Accuracy and Cost (sum of loss)\n",
    "\n",
    "![](https://www.researchgate.net/publication/367393140/figure/fig4/AS:11431281114710300@1674648981676/Confusion-matrix-Precision-Recall-Accuracy-and-F1-score.jpg)\n",
    "\n",
    "* MSE\n",
    "\n",
    "$$loss=\\sqrt{(\\hat{Y} - Y)^2}$$\n",
    "\n",
    "* CategoricalCrossentropy\n",
    "\n",
    "$$loss=-(Y*log(\\hat{Y}) + (1-Y)*log(1-\\hat{Y}))$$\n",
    "\n",
    "* Cost\n",
    "\n",
    "$$C=\\cfrac{1}{m}\\sum{L(\\hat{Y},Y)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15630c82-8488-4b1c-9563-d9f53bdb3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_train.shape[0]\n",
    "loss = np.average(np.square(Y_hat - Y_train) ** 0.5)\n",
    "acc = (np.argmax(Y_hat, axis=1) == np.argmax(Y_train, axis=1)).sum() / m\n",
    "print(\"loss:\", loss)\n",
    "print(\"acc:\", acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2619ed40-2ffb-437b-99aa-03979bffffd7",
   "metadata": {},
   "source": [
    "## Gradient Descent (Optimizers)\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{W}^{[l]}_ {(h_ {l-1},h_ {l})}&=\\mathbb{W}^{[l]}_ {(h_ {l-1},h_ {l})}-\\alpha\\cfrac{∂C}{∂W}^{[l]}_ {(h_ {l-1},h_ {l})}\n",
    "\\\\\n",
    "\\\\\n",
    "\\mathbb{B}^{[l]}_ {(1,h_ {l})}&=\\mathbb{B}^{[l]}_ {(1,h_ {l})}-\\alpha\\cfrac{∂C}{∂B}^{[l]}_ {(1,h_ {l})}\n",
    "\\end{aligned}$$\n",
    "\n",
    "## Backward Propagation\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*CnoTckCQhlXMDjjR8_n7IQ.gif)\n",
    "\n",
    "### Output Layer (Softmax)\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\cfrac{∂C}{∂Z}^{[o]}_ {(m,k)}&=\\hat{Y}_ {(m,k)}-Y_ {(m,k)}\\\\\n",
    "\\\\\n",
    "\\cfrac{∂C}{∂W}^{[o]}_ {(h_ {l},k)}&=\\cfrac{1}{m}X^{[l]T}_ {(h_ {l},m)}{\\ \\cdot\\ }\\cfrac{∂C}{∂Z}^{[o]}_ {(m,k)}\\\\\n",
    "\\\\\n",
    "\\cfrac{∂C}{∂B}^{[o]}_ {(1,k)}&=\\cfrac{1}{m}\\sum{(\\cfrac{∂C}{∂Z}^{[o]}_ {(m,k)})}\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "### Hidden Layer\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\cfrac{∂C}{∂Z}^{[l]}_ {(m,h_ {l})}&=[\\cfrac{∂C}{∂Z}^{[o]}_ {(m,k)}{\\ \\cdot\\ }W^{[o]T}_ {(k,h_ {l})}]\\times[\\sigma'^{[l]}(Z^{[l]}_ {(m,h_ {l})})]\\\\\n",
    "\\\\\n",
    "\\cfrac{∂C}{∂W}^{[l]}_ {(h_ {l-1},h_ {l})}&=\\cfrac{1}{m}X^{[l]T}_ {(h_ {l-1},m)}{\\ \\cdot\\ }\\cfrac{∂C}{∂Z}^{[o]}_ {(m,h_ {l})}\\\\\n",
    "\\\\\n",
    "\\cfrac{∂C}{∂B}^{[o]}_ {(1,h_ {l})}&=\\cfrac{1}{m}\\sum{(\\cfrac{∂C}{∂Z}^{[l]}_ {(m,h_ {l})})}\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "$\\quad$\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\cfrac{∂C}{∂Z}^{[l-1]}_ {(m,h_ {l-1})}&=[\\cfrac{∂C}{∂Z}^{[l]}_ {(m,h_ {l})}{\\ \\cdot\\ }W^{[l]T}_ {(h_ {l},h_ {l-1})}]\\times[\\sigma'^{[l-1]}(Z^{[l-1]}_ {(m,h_ {l-1})})]\\\\\n",
    "\\\\\n",
    "\\cfrac{∂C}{∂W}^{[l-1]}_ {(h_ {l-2},h_ {l-1})}&=\\cfrac{1}{m}X^{[l-2]T}_ {(h_ {l-2},m)}{\\ \\cdot\\ }\\cfrac{∂C}{∂Z}^{[l]}_ {(m,h_ {l-1})}\\\\\n",
    "\\\\\n",
    "\\cfrac{∂C}{∂B}^{[l-1]}_ {(1,h_ {l-1})}&=\\cfrac{1}{m}\\sum{(\\cfrac{∂C}{∂Z}^{[l-1]}_ {(m,h_ {l-1})})}\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\\\\n",
    "&\\quad \\quad \\quad \\quad \\quad \\quad \\bullet\\\\\n",
    "&\\quad \\quad \\quad \\quad \\quad \\quad \\bullet\\\\\n",
    "&\\quad \\quad \\quad \\quad \\quad \\quad \\bullet\\\\\n",
    "\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\cfrac{∂C}{∂Z}^{[2]}_ {(m,h_ {2})}&=[\\cfrac{∂C}{∂Z}^{[3]}_ {(m,h_ {3})}{\\ \\cdot\\ }W^{[3]T}_ {(h_ {3},h_ {2})}]\\times[\\sigma'^{[2]}(Z^{[2]}_ {(m,h_ {2})})]\\\\\n",
    "\\\\\n",
    "\\cfrac{∂C}{∂W}^{[2]}_ {(h_ {1},h_ {2})}&=\\cfrac{1}{m}X^{[1]T}_ {(h_ {1},m)}{\\ \\cdot\\ }\\cfrac{∂C}{∂Z}^{[2]}_ {(m,h_ {2})}\\\\\n",
    "\\\\\n",
    "\\cfrac{∂C}{∂B}^{[2]}_ {(1,h_ {2})}&=\\cfrac{1}{m}\\sum{(\\cfrac{∂C}{∂Z}^{[2]}_ {(m,h_ {2})})}\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "### Input Layer\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\cfrac{∂C}{∂Z}^{[1]}_ {(m,h_ {1})}&=[\\cfrac{∂C}{∂Z}^{[2]}_ {(m,h_ {2})}{\\ \\cdot\\ }W^{[2]T}_ {(h_ {2},h_ {1})}]\\times[\\sigma'^{[1]}(Z^{[1]}_ {(m,h_ {1})})]\\\\\n",
    "\\\\\n",
    "\\cfrac{∂C}{∂W}^{[1]}_ {(n,h_ {1})}&=\\cfrac{1}{m}X^{[0]T}_ {(n,m)}{\\ \\cdot\\ }\\cfrac{∂C}{∂Z}^{[1]}_ {(m,h_ {1})}\\\\\n",
    "\\\\\n",
    "\\cfrac{∂C}{∂B}^{[1]}_ {(1,h_ {1})}&=\\cfrac{1}{m}\\sum{(\\cfrac{∂C}{∂Z}^{[1]}_ {(m,h_ {1})})}\\\\\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc0fc9-10bc-4277-af62-e16955302b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 3\n",
    "dZ3 = softmax_backward(Y_hat, Y_train)\n",
    "dW3 = np.dot(X2.T, dZ3) / m\n",
    "db3 = np.sum(dZ3, axis=0) / m\n",
    "# layer 2\n",
    "dZ2 = np.dot(dZ3, W3.T) * relu_backward(Z2)\n",
    "dW2 = np.dot(X1.T, dZ2) / m\n",
    "db2 = np.sum(dZ2, axis=0) / m\n",
    "# layer 1\n",
    "dZ1 = np.dot(dZ2, W2.T) * relu_backward(Z1)\n",
    "dW1 = np.dot(X_train.T, dZ1) / m\n",
    "db1 = np.sum(dZ1, axis=0) / m\n",
    "\n",
    "lr = 1e-3\n",
    "W1 = W1 - lr * dW1\n",
    "W2 = W2 - lr * dW2\n",
    "W3 = W3 - lr * dW3\n",
    "b1 = b1 - lr * db1\n",
    "b2 = b2 - lr * db2\n",
    "b3 = b3 - lr * db3\n",
    "\n",
    "# complete a epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7bf1ab-e68c-499f-a0e0-a474cb9b0e8a",
   "metadata": {},
   "source": [
    "## Train in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dfbe22-56dc-4881-9b76-0e6b785deec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "batch_size = 16 # batch_size=1 -> 盲人摸象\n",
    "\n",
    "# weight and bias\n",
    "W1 = np.random.randn(784, 128) * 0.1\n",
    "b1 = np.random.randn(128) * 0.1\n",
    "W2 = np.random.randn(128, 64) * 0.1\n",
    "b2 = np.random.randn(64) * 0.1\n",
    "W3 = np.random.randn(64, 10) * 0.1\n",
    "b3 = np.random.randn(10) * 0.1\n",
    "\n",
    "# set batch\n",
    "batch_iteration = int(X_train.shape[0] / batch_size)\n",
    "batch_iteration = batch_iteration + 1 if X_train.shape[0] % batch_size > 0 else batch_iteration\n",
    "for i in range(epochs):\n",
    "    loss = acc = val_loss = val_acc = 0\n",
    "    for j in range(batch_iteration):\n",
    "        batch_X = X_train[j*batch_size:(j+1)*batch_size]\n",
    "        batch_Y = Y_train[j*batch_size:(j+1)*batch_size]\n",
    "        m = batch_X.shape[0]\n",
    "        # forward propogation\n",
    "        Z1 = np.dot(batch_X, W1) + b1\n",
    "        X1 = relu(Z1)\n",
    "        Z2 = np.dot(X1, W2) + b2\n",
    "        X2 = relu(Z2)\n",
    "        Z3 = np.dot(X2, W3) + b3\n",
    "        Y_hat = softmax(Z3)\n",
    "        # batch loss, acc\n",
    "        batch_loss = np.average(np.square(Y_hat - batch_Y) ** 0.5)\n",
    "        batch_acc = (np.argmax(Y_hat, axis=1) == np.argmax(batch_Y, axis=1)).sum() / m\n",
    "        loss = loss + batch_loss / batch_iteration\n",
    "        acc = acc + batch_acc / batch_iteration\n",
    "        # backward propogation\n",
    "        # layer 3\n",
    "        dZ3 = softmax_backward(Y_hat, batch_Y)\n",
    "        dW3 = np.dot(X2.T, dZ3) / m\n",
    "        db3 = np.sum(dZ3, axis=0) / m\n",
    "        # layer 2\n",
    "        dZ2 = np.dot(dZ3, W3.T) * relu_backward(Z2)\n",
    "        dW2 = np.dot(X1.T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0) / m\n",
    "        # layer 1\n",
    "        dZ1 = np.dot(dZ2, W2.T) * relu_backward(Z1)\n",
    "        dW1 = np.dot(batch_X.T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0) / m\n",
    "        # gradient descent\n",
    "        W1 = W1 - lr * dW1\n",
    "        W2 = W2 - lr * dW2\n",
    "        W3 = W3 - lr * dW3\n",
    "        b1 = b1 - lr * db1\n",
    "        b2 = b2 - lr * db2\n",
    "        b3 = b3 - lr * db3\n",
    "    # loss, acc, val_loss, val_acc\n",
    "    Z1 = np.dot(X_test, W1) + b1\n",
    "    X1 = relu(Z1)\n",
    "    Z2 = np.dot(X1, W2) + b2\n",
    "    X2 = relu(Z2)\n",
    "    Z3 = np.dot(X2, W3) + b3\n",
    "    Y_hat = softmax(Z3)\n",
    "    val_loss = np.average(np.square(Y_hat - Y_test) ** 0.5)\n",
    "    val_acc = (np.argmax(Y_hat, axis=1) == np.argmax(Y_test, axis=1)).sum() / Y_test.shape[0]\n",
    "    print(\"Epoch {: 5d}/{}\\t- loss: {:.4f} - acc: {:.4f} - val_loss: {:.4f} - val_acc: {:.4f}\".format(i+1, epochs, loss, acc, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c1aab-a7a0-464a-a572-9d07933db60f",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a4fc1-548f-43d6-8aed-3aa087a99569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "i_test = random.randint(0, 2000)\n",
    "img = X_test[i_test]\n",
    "Z1 = np.dot(np.array([img]), W1) + b1\n",
    "X1 = relu(Z1)\n",
    "Z2 = np.dot(X1, W2) + b2\n",
    "X2 = relu(Z2)\n",
    "Z3 = np.dot(X2, W3) + b3\n",
    "Y_hat = softmax(Z3)\n",
    "predict = label_dict[Y_hat[0].argmax()]\n",
    "print(\"predict:\", predict)\n",
    "print(\"answer:\", label_dict[Y_test[i_test].argmax()])\n",
    "Image.fromarray(img.reshape(28, 28).astype(np.uint8)).resize((140, 140)).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
